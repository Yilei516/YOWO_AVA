{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "alike-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import sys\n",
    "import logging\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import dataset\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from opts import parse_opts\n",
    "from utils import *\n",
    "from cfg import parse_cfg\n",
    "from region_loss import RegionLoss\n",
    "\n",
    "from model import YOWO, get_fine_tuning_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "portuguese-auction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "class options:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.dataset = 'ucf101-24' \n",
    "        self.data_cfg = 'cfg/ucf24.data' \n",
    "        self.cfg_file = 'cfg/ucf24.cfg' \n",
    "        self.n_classes = 24 \n",
    "        self.backbone_3d = 'resnext101' \n",
    "        self.backbone_3d_weights = 'weights/resnext-101-kinetics.pth' \n",
    "        self.backbone_2d = 'darknet' \n",
    "        self.backbone_2d_weights = 'weights/yolo.weights' \n",
    "        self.resume_path = 'backup'\n",
    "        self.freeze_backbone_2d = False\n",
    "        self.freeze_backbone_3d = False\n",
    "\n",
    "opt = options()\n",
    "# which dataset to use\n",
    "dataset_use   = opt.dataset\n",
    "assert dataset_use == 'ucf101-24' or dataset_use == 'jhmdb-21', 'invalid dataset'\n",
    "# path for dataset of training and validation\n",
    "datacfg       = opt.data_cfg\n",
    "# path for cfg file\n",
    "cfgfile       = opt.cfg_file\n",
    "\n",
    "data_options  = read_data_cfg(datacfg)\n",
    "net_options   = parse_cfg(cfgfile)[0]\n",
    "\n",
    "# obtain list for training and testing\n",
    "basepath      = data_options['base']\n",
    "trainlist     = data_options['train']\n",
    "testlist      = data_options['valid']\n",
    "backupdir     = data_options['backup']\n",
    "# number of training samples\n",
    "nsamples      = file_lines(trainlist)\n",
    "\n",
    "gpu_ids = list(range(torch.cuda.device_count()))\n",
    "gpus = ','.join([str(g) for g in gpu_ids]) # gpus          = data_options['gpus']  # e.g. 0,1,2,3\n",
    "ngpus = len(gpu_ids) # ngpus         = len(gpus.split(','))\n",
    "num_workers   = int(data_options['num_workers'])\n",
    "\n",
    "net_options['batch'] *= ngpus\n",
    "batch_size    = int(net_options['batch'])*ngpus\n",
    "clip_duration = int(net_options['clip_duration'])\n",
    "max_batches   = int(net_options['max_batches'])\n",
    "learning_rate = float(net_options['learning_rate'])\n",
    "momentum      = float(net_options['momentum'])\n",
    "decay         = float(net_options['decay'])\n",
    "steps         = [float(step) for step in net_options['steps'].split(',')]\n",
    "scales        = [float(scale) for scale in net_options['scales'].split(',')]\n",
    "\n",
    "# loss parameters\n",
    "loss_options               = parse_cfg(cfgfile)[1]\n",
    "region_loss                = RegionLoss()\n",
    "anchors                    = loss_options['anchors'].split(',')\n",
    "region_loss.anchors        = [float(i) for i in anchors]\n",
    "region_loss.num_classes    = int(loss_options['classes'])\n",
    "region_loss.num_anchors    = int(loss_options['num'])\n",
    "region_loss.anchor_step    = len(region_loss.anchors)//region_loss.num_anchors\n",
    "region_loss.object_scale   = float(loss_options['object_scale'])\n",
    "region_loss.noobject_scale = float(loss_options['noobject_scale'])\n",
    "region_loss.class_scale    = float(loss_options['class_scale'])\n",
    "region_loss.coord_scale    = float(loss_options['coord_scale'])\n",
    "region_loss.batch          = batch_size\n",
    "        \n",
    "#Train parameters\n",
    "max_epochs    = max_batches*batch_size//nsamples+1\n",
    "use_cuda      = True\n",
    "seed          = int(time.time())\n",
    "eps           = 1e-5\n",
    "best_fscore   = 0 # initialize best fscore\n",
    "\n",
    "# Test parameters\n",
    "nms_thresh    = 0.4\n",
    "iou_thresh    = 0.5\n",
    "\n",
    "if not os.path.exists(backupdir):\n",
    "    os.mkdir(backupdir)\n",
    "    \n",
    "torch.manual_seed(seed)\n",
    "if use_cuda:\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = gpus\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "# Create model\n",
    "model = YOWO(opt)\n",
    "\n",
    "model       = model.cuda()\n",
    "model       = nn.DataParallel(model, device_ids=gpu_ids) # in multi-gpu case\n",
    "model.seen  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "political-acrylic",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bigger-mexico",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sacred-attitude",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "english-girlfriend",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.nn.parallel.data_parallel.DataParallel"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "congressional-contrast",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.device_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "photographic-bhutan",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): YOWO(\n",
      "    (backbone_2d): Darknet(\n",
      "      (models): ModuleList(\n",
      "        (0): Sequential(\n",
      "          (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky1): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (2): Sequential(\n",
      "          (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (4): Sequential(\n",
      "          (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky3): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (5): Sequential(\n",
      "          (conv4): Conv2d(128, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky4): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (6): Sequential(\n",
      "          (conv5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky5): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (8): Sequential(\n",
      "          (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (9): Sequential(\n",
      "          (conv7): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky7): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (10): Sequential(\n",
      "          (conv8): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky8): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (12): Sequential(\n",
      "          (conv9): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (13): Sequential(\n",
      "          (conv10): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn10): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky10): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (14): Sequential(\n",
      "          (conv11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky11): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (15): Sequential(\n",
      "          (conv12): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn12): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky12): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (16): Sequential(\n",
      "          (conv13): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn13): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky13): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (17): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "        (18): Sequential(\n",
      "          (conv14): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn14): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky14): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (19): Sequential(\n",
      "          (conv15): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky15): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (20): Sequential(\n",
      "          (conv16): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn16): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky16): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (21): Sequential(\n",
      "          (conv17): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn17): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky17): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (22): Sequential(\n",
      "          (conv18): Conv2d(512, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn18): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky18): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (23): Sequential(\n",
      "          (conv19): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn19): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky19): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (24): Sequential(\n",
      "          (conv20): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn20): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky20): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (25): EmptyModule()\n",
      "        (26): Sequential(\n",
      "          (conv21): Conv2d(512, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn21): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky21): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (27): Reorg()\n",
      "        (28): EmptyModule()\n",
      "        (29): Sequential(\n",
      "          (conv22): Conv2d(1280, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn22): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (leaky22): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "        )\n",
      "        (30): Sequential(\n",
      "          (conv23): Conv2d(1024, 425, kernel_size=(1, 1), stride=(1, 1))\n",
      "        )\n",
      "        (31): RegionLoss()\n",
      "      )\n",
      "      (loss): RegionLoss()\n",
      "    )\n",
      "    (backbone_3d): ResNeXt(\n",
      "      (conv1): Conv3d(3, 64, kernel_size=(7, 7, 7), stride=(1, 2, 2), padding=(3, 3, 3), bias=False)\n",
      "      (bn1): BatchNorm3d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (maxpool): MaxPool3d(kernel_size=(3, 3, 3), stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "      (layer1): Sequential(\n",
      "        (0): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(64, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv3d(64, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "            (1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(256, 128, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(128, 128, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(128, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer2): Sequential(\n",
      "        (0): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(256, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "            (1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(512, 256, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(256, 256, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(256, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer3): Sequential(\n",
      "        (0): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(512, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "            (1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (3): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (4): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (5): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (6): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (7): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (8): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (9): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (10): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (11): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (12): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (13): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (14): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (15): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (16): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (17): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (18): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (19): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (20): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (21): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (22): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 512, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(512, 512, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(512, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "      (layer4): Sequential(\n",
      "        (0): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(1024, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(2, 2, 2), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "          (downsample): Sequential(\n",
      "            (0): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(2, 2, 2), bias=False)\n",
      "            (1): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          )\n",
      "        )\n",
      "        (1): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "        (2): ResNeXtBottleneck(\n",
      "          (conv1): Conv3d(2048, 1024, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn1): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv2): Conv3d(1024, 1024, kernel_size=(3, 3, 3), stride=(1, 1, 1), padding=(1, 1, 1), groups=32, bias=False)\n",
      "          (bn2): BatchNorm3d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (conv3): Conv3d(1024, 2048, kernel_size=(1, 1, 1), stride=(1, 1, 1), bias=False)\n",
      "          (bn3): BatchNorm3d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "          (relu): ReLU(inplace=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (cfam): CFAMBlock(\n",
      "      (conv_bn_relu1): Sequential(\n",
      "        (0): Conv2d(2473, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv_bn_relu2): Sequential(\n",
      "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (sc): CAM_Module(\n",
      "        (softmax): Softmax(dim=-1)\n",
      "      )\n",
      "      (conv_bn_relu3): Sequential(\n",
      "        (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU()\n",
      "      )\n",
      "      (conv_out): Sequential(\n",
      "        (0): Dropout2d(p=0.1, inplace=False)\n",
      "        (1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (conv_final): Conv2d(1024, 145, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "closed-mountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = get_fine_tuning_parameters(model, opt)\n",
    "optimizer = optim.SGD(parameters, lr=learning_rate/batch_size, momentum=momentum, dampening=0, weight_decay=decay*batch_size)\n",
    "\n",
    "kwargs = {'num_workers': num_workers, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "optimum-digit",
   "metadata": {},
   "outputs": [],
   "source": [
    "chkpt_core = 'yowo_' + opt.dataset + '_' + str(clip_duration) + 'f'\n",
    "chkpt = sorted([c for c in os.listdir(opt.resume_path) if chkpt_core in c and 'checkpoint.pth' in c])\n",
    "max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "amazing-input",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['yowo_ucf101-24_16f_10_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_11_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_12_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_13_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_14_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_15_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_16_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_17_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_18_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_19_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_1_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_20_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_2_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_3_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_4_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_5_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_6_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_7_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_8_checkpoint.pth',\n",
       " 'yowo_ucf101-24_16f_9_checkpoint.pth']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chkpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "optical-contents",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===================================================================\n",
      "loading checkpoint backup/yowo_ucf101-24_16f_20_checkpoint.pth\n",
      "Loaded model fscore:  0.8775868513544806\n",
      "===================================================================\n"
     ]
    }
   ],
   "source": [
    "if opt.resume_path:\n",
    "    print(\"===================================================================\")\n",
    "    if '.pth' in opt.resume_path:\n",
    "        chkpt = opt.resume_path\n",
    "    else:\n",
    "        chkpt_core = 'yowo_' + opt.dataset + '_' + str(clip_duration) + 'f'\n",
    "        chkpt = [c for c in os.listdir(opt.resume_path) if chkpt_core in c and 'checkpoint.pth' in c]\n",
    "        max_len = max([len(c) for c in chkpt])\n",
    "        chkpt = sorted([c for c in chkpt if len(c)==max_len])\n",
    "        if chkpt:\n",
    "            chkpt = os.path.join(opt.resume_path,chkpt[-1])\n",
    "    if chkpt:\n",
    "        print('loading checkpoint {}'.format(chkpt))\n",
    "        checkpoint = torch.load(chkpt)\n",
    "        opt.begin_epoch = checkpoint['epoch'] + 1\n",
    "        best_fscore = checkpoint['fscore']\n",
    "        model.load_state_dict(checkpoint['state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        model.seen = checkpoint['epoch'] * nsamples\n",
    "        print(\"Loaded model fscore: \", checkpoint['fscore'])\n",
    "        print(\"===================================================================\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "arbitrary-ready",
   "metadata": {},
   "outputs": [],
   "source": [
    "region_loss.seen  = model.seen\n",
    "processed_batches = model.seen//batch_size\n",
    "\n",
    "init_width        = int(net_options['width'])\n",
    "init_height       = int(net_options['height'])\n",
    "init_epoch        = model.seen//nsamples \n",
    "\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, batch):\n",
    "    lr = learning_rate\n",
    "    for i in range(len(steps)):\n",
    "        scale = scales[i] if i < len(scales) else 1\n",
    "        if batch >= steps[i]:\n",
    "            lr = lr * scale\n",
    "            if batch == steps[i]:\n",
    "                break\n",
    "        else:\n",
    "            break\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr/batch_size\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forbidden-turkish",
   "metadata": {},
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "confident-juvenile",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "prerequisite-banks",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import RandomSampler\n",
    "writer = SummaryWriter()\n",
    "n_sample_from = 10\n",
    "def train(epoch):\n",
    "    global processed_batches\n",
    "    t0 = time.time()\n",
    "    cur_model = model.module\n",
    "    region_loss.l_x.reset()\n",
    "    region_loss.l_y.reset()\n",
    "    region_loss.l_w.reset()\n",
    "    region_loss.l_h.reset()\n",
    "    region_loss.l_conf.reset()\n",
    "    region_loss.l_cls.reset()\n",
    "    region_loss.l_total.reset()\n",
    "    train_dataset = dataset.listDataset(basepath, trainlist, dataset_use=dataset_use, shape=(init_width, init_height),\n",
    "                                        shuffle=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.ToTensor(),\n",
    "                                        ]), \n",
    "                                        train=True, \n",
    "                                        seen=cur_model.seen,\n",
    "                                        batch_size=batch_size,\n",
    "                                        clip_duration=clip_duration,\n",
    "                                        num_workers=num_workers)\n",
    "    rs = RandomSampler(train_dataset,replacement=True, num_samples=int(len(train_dataset)/n_sample_from))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,sampler=rs,\n",
    "                                               batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    lr = adjust_learning_rate(optimizer, processed_batches)\n",
    "    total_batch = len(train_loader)\n",
    "    logging('training at epoch %d, lr %f' % (epoch, lr))\n",
    "    logging('total # of batches %d' % (total_batch))\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        lr = adjust_learning_rate(optimizer, processed_batches)\n",
    "        processed_batches = processed_batches + 1\n",
    "\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        region_loss.seen = region_loss.seen + data.data.size(0)\n",
    "        loss = region_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx%20 == 0:\n",
    "            logging(f'===== epoch: {epoch}, batch: {batch_idx+1}/{total_batch}, lr:{lr}, loss: {loss.item()} =====')\n",
    "        \n",
    "        # save result every 1000 batches\n",
    "        if processed_batches % 500 == 0: # From time to time, reset averagemeters to see improvements\n",
    "            region_loss.l_x.reset()\n",
    "            region_loss.l_y.reset()\n",
    "            region_loss.l_w.reset()\n",
    "            region_loss.l_h.reset()\n",
    "            region_loss.l_conf.reset()\n",
    "            region_loss.l_cls.reset()\n",
    "            region_loss.l_total.reset()\n",
    "        \n",
    "        writer.add_scalar('Loss/train', loss.item(), batch_idx)\n",
    "        writer.add_scalar('Training/lr', lr, batch_idx)\n",
    "\n",
    "    t1 = time.time()\n",
    "    logging('trained with %f samples/s' % (len(train_loader.dataset)/(t1-t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floral-blend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-27 17:09:14 training at epoch 0, lr 0.000006\n",
      "2021-03-27 17:09:14 total # of batches 1408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/ylwu/anaconda3/envs/YW/lib/python3.9/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-27 17:14:04 ===== epoch: 0, batch: 1/1408, lr:6.25e-06, loss: 8.133235931396484 =====\n",
      "2021-03-27 17:15:36 ===== epoch: 0, batch: 21/1408, lr:6.25e-06, loss: 5.722564220428467 =====\n"
     ]
    }
   ],
   "source": [
    "train(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-canberra",
   "metadata": {},
   "source": [
    "### WandB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "entertaining-kingdom",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from torch.utils.data import RandomSampler\n",
    "n_sample_from = 10\n",
    "batch_size = 6\n",
    "def train(epoch):\n",
    "    wandb_id = '2ufkp25t' #wandb.util.generate_id()\n",
    "    print(wandb_id)\n",
    "    wandb.init(project='YOWO_UCF101-24', entity='wuyilei516',config=vars(opt),id=wandb_id, resume=\"allow\")\n",
    "    wandb.watch(model)\n",
    "    \n",
    "    global processed_batches\n",
    "    t0 = time.time()\n",
    "    cur_model = model.module\n",
    "    region_loss.l_x.reset()\n",
    "    region_loss.l_y.reset()\n",
    "    region_loss.l_w.reset()\n",
    "    region_loss.l_h.reset()\n",
    "    region_loss.l_conf.reset()\n",
    "    region_loss.l_cls.reset()\n",
    "    region_loss.l_total.reset()\n",
    "    train_dataset = dataset.listDataset(basepath, trainlist, dataset_use=dataset_use, shape=(init_width, init_height),\n",
    "                                        shuffle=True,\n",
    "                                        transform=transforms.Compose([\n",
    "                                            transforms.ToTensor(),\n",
    "                                        ]), \n",
    "                                        train=True, \n",
    "                                        seen=cur_model.seen,\n",
    "                                        batch_size=batch_size,\n",
    "                                        clip_duration=clip_duration,\n",
    "                                        num_workers=num_workers)\n",
    "    rs = RandomSampler(train_dataset,replacement=True, num_samples=int(len(train_dataset)/n_sample_from))\n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset,sampler=rs,\n",
    "                                               batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    lr = adjust_learning_rate(optimizer, processed_batches)\n",
    "    total_batch = len(train_loader)\n",
    "    logging('training at epoch %d, lr %f' % (epoch, lr))\n",
    "    logging('total # of batches %d' % (total_batch))\n",
    "\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        \n",
    "        lr = adjust_learning_rate(optimizer, processed_batches)\n",
    "        processed_batches = processed_batches + 1\n",
    "\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        region_loss.seen = region_loss.seen + data.data.size(0)\n",
    "        loss = region_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx%20 == 0:\n",
    "            logging(f'===== epoch: {epoch}, batch: {batch_idx+1}/{total_batch}, lr:{lr}, loss: {loss.item()} =====')\n",
    "            wandb.log({\"loss\": loss})\n",
    "            wandb.log({\"lr\": lr})\n",
    "            \n",
    "        # save result every 1000 batches\n",
    "        if processed_batches % 500 == 0: # From time to time, reset averagemeters to see improvements\n",
    "            region_loss.l_x.reset()\n",
    "            region_loss.l_y.reset()\n",
    "            region_loss.l_w.reset()\n",
    "            region_loss.l_h.reset()\n",
    "            region_loss.l_conf.reset()\n",
    "            region_loss.l_cls.reset()\n",
    "            region_loss.l_total.reset()\n",
    "\n",
    "    t1 = time.time()\n",
    "    logging('trained with %f samples/s' % (len(train_loader.dataset)/(t1-t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "returning-lindsay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2ufkp25t\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mvector_cv\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "                Tracking run with wandb version 0.10.23<br/>\n",
       "                Resuming run <strong style=\"color:#cdcd00\">rural-jazz-6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n",
       "                Project page: <a href=\"https://wandb.ai/wuyilei516/YOWO_UCF101-24\" target=\"_blank\">https://wandb.ai/wuyilei516/YOWO_UCF101-24</a><br/>\n",
       "                Run page: <a href=\"https://wandb.ai/wuyilei516/YOWO_UCF101-24/runs/2ufkp25t\" target=\"_blank\">https://wandb.ai/wuyilei516/YOWO_UCF101-24/runs/2ufkp25t</a><br/>\n",
       "                Run data is saved locally in <code>/scratch/ssd002/home/ylwu/YOWO/wandb/run-20210327_212741-2ufkp25t</code><br/><br/>\n",
       "            "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-27 21:27:42 training at epoch 1, lr 0.000006\n",
      "2021-03-27 21:27:42 total # of batches 5631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/h/ylwu/anaconda3/envs/YW/lib/python3.9/site-packages/torch/utils/data/dataloader.py:474: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(_create_warning_msg(\n",
      "/h/ylwu/anaconda3/envs/YW/lib/python3.9/site-packages/torch/nn/modules/module.py:795: UserWarning: Using a non-full backward hook when the forward contains multiple autograd Nodes is deprecated and will be removed in future versions. This hook will be missing some grad_input. Please use register_full_backward_hook to get the documented behavior.\n",
      "  warnings.warn(\"Using a non-full backward hook when the forward contains multiple autograd Nodes \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-27 21:27:49 ===== epoch: 1, batch: 1/5631, lr:6.25e-06, loss: 0.9585931301116943 =====\n",
      "2021-03-27 21:28:14 ===== epoch: 1, batch: 21/5631, lr:6.25e-06, loss: 4.0296478271484375 =====\n",
      "2021-03-27 21:28:39 ===== epoch: 1, batch: 41/5631, lr:6.25e-06, loss: 1.774702787399292 =====\n",
      "2021-03-27 21:29:04 ===== epoch: 1, batch: 61/5631, lr:6.25e-06, loss: 1.8747456073760986 =====\n",
      "2021-03-27 21:29:29 ===== epoch: 1, batch: 81/5631, lr:6.25e-06, loss: 1.272158145904541 =====\n",
      "2021-03-27 21:29:53 ===== epoch: 1, batch: 101/5631, lr:6.25e-06, loss: 3.947875499725342 =====\n",
      "2021-03-27 21:30:18 ===== epoch: 1, batch: 121/5631, lr:6.25e-06, loss: 1.9794793128967285 =====\n",
      "2021-03-27 21:30:43 ===== epoch: 1, batch: 141/5631, lr:6.25e-06, loss: 1.2886033058166504 =====\n",
      "2021-03-27 21:31:08 ===== epoch: 1, batch: 161/5631, lr:6.25e-06, loss: 3.277496576309204 =====\n",
      "2021-03-27 21:31:33 ===== epoch: 1, batch: 181/5631, lr:6.25e-06, loss: 2.176551580429077 =====\n",
      "2021-03-27 21:31:58 ===== epoch: 1, batch: 201/5631, lr:6.25e-06, loss: 2.466945171356201 =====\n",
      "2021-03-27 21:32:22 ===== epoch: 1, batch: 221/5631, lr:6.25e-06, loss: 0.8233121037483215 =====\n",
      "2021-03-27 21:32:47 ===== epoch: 1, batch: 241/5631, lr:6.25e-06, loss: 1.136475682258606 =====\n",
      "2021-03-27 21:33:12 ===== epoch: 1, batch: 261/5631, lr:6.25e-06, loss: 2.5660178661346436 =====\n",
      "2021-03-27 21:33:37 ===== epoch: 1, batch: 281/5631, lr:6.25e-06, loss: 0.9376480579376221 =====\n",
      "2021-03-27 21:34:02 ===== epoch: 1, batch: 301/5631, lr:6.25e-06, loss: 1.645371913909912 =====\n",
      "2021-03-27 21:34:26 ===== epoch: 1, batch: 321/5631, lr:6.25e-06, loss: 1.4094438552856445 =====\n",
      "2021-03-27 21:34:51 ===== epoch: 1, batch: 341/5631, lr:6.25e-06, loss: 1.7853901386260986 =====\n",
      "2021-03-27 21:35:16 ===== epoch: 1, batch: 361/5631, lr:6.25e-06, loss: 1.4357439279556274 =====\n",
      "2021-03-27 21:35:41 ===== epoch: 1, batch: 381/5631, lr:6.25e-06, loss: 2.150503635406494 =====\n",
      "2021-03-27 21:36:06 ===== epoch: 1, batch: 401/5631, lr:6.25e-06, loss: 2.1180522441864014 =====\n",
      "2021-03-27 21:36:30 ===== epoch: 1, batch: 421/5631, lr:6.25e-06, loss: 3.530388355255127 =====\n",
      "2021-03-27 21:36:55 ===== epoch: 1, batch: 441/5631, lr:6.25e-06, loss: 1.9846742153167725 =====\n",
      "2021-03-27 21:37:20 ===== epoch: 1, batch: 461/5631, lr:6.25e-06, loss: 1.5741002559661865 =====\n",
      "2021-03-27 21:37:45 ===== epoch: 1, batch: 481/5631, lr:6.25e-06, loss: 1.45430326461792 =====\n",
      "2021-03-27 21:38:10 ===== epoch: 1, batch: 501/5631, lr:6.25e-06, loss: 1.8893781900405884 =====\n",
      "2021-03-27 21:38:34 ===== epoch: 1, batch: 521/5631, lr:6.25e-06, loss: 3.01155161857605 =====\n",
      "2021-03-27 21:38:59 ===== epoch: 1, batch: 541/5631, lr:6.25e-06, loss: 1.855125069618225 =====\n",
      "2021-03-27 21:39:24 ===== epoch: 1, batch: 561/5631, lr:6.25e-06, loss: 0.731027364730835 =====\n",
      "2021-03-27 21:39:49 ===== epoch: 1, batch: 581/5631, lr:6.25e-06, loss: 2.8632116317749023 =====\n",
      "2021-03-27 21:40:14 ===== epoch: 1, batch: 601/5631, lr:6.25e-06, loss: 2.0393857955932617 =====\n",
      "2021-03-27 21:40:38 ===== epoch: 1, batch: 621/5631, lr:6.25e-06, loss: 1.121213674545288 =====\n",
      "2021-03-27 21:41:03 ===== epoch: 1, batch: 641/5631, lr:6.25e-06, loss: 0.5645787715911865 =====\n",
      "2021-03-27 21:41:28 ===== epoch: 1, batch: 661/5631, lr:6.25e-06, loss: 0.3779626786708832 =====\n",
      "2021-03-27 21:41:52 ===== epoch: 1, batch: 681/5631, lr:6.25e-06, loss: 1.70814049243927 =====\n",
      "2021-03-27 21:42:17 ===== epoch: 1, batch: 701/5631, lr:6.25e-06, loss: 1.2742654085159302 =====\n",
      "2021-03-27 21:42:42 ===== epoch: 1, batch: 721/5631, lr:6.25e-06, loss: 2.046748638153076 =====\n",
      "2021-03-27 21:43:06 ===== epoch: 1, batch: 741/5631, lr:6.25e-06, loss: 3.2171483039855957 =====\n",
      "2021-03-27 21:43:31 ===== epoch: 1, batch: 761/5631, lr:6.25e-06, loss: 1.193764567375183 =====\n",
      "2021-03-27 21:43:56 ===== epoch: 1, batch: 781/5631, lr:6.25e-06, loss: 4.135099411010742 =====\n",
      "2021-03-27 21:44:21 ===== epoch: 1, batch: 801/5631, lr:6.25e-06, loss: 1.6597118377685547 =====\n",
      "2021-03-27 21:44:46 ===== epoch: 1, batch: 821/5631, lr:6.25e-06, loss: 2.5456044673919678 =====\n",
      "2021-03-27 21:45:10 ===== epoch: 1, batch: 841/5631, lr:6.25e-06, loss: 0.4958626329898834 =====\n",
      "2021-03-27 21:45:35 ===== epoch: 1, batch: 861/5631, lr:6.25e-06, loss: 1.0376286506652832 =====\n",
      "2021-03-27 21:46:00 ===== epoch: 1, batch: 881/5631, lr:6.25e-06, loss: 1.2937079668045044 =====\n",
      "2021-03-27 21:46:25 ===== epoch: 1, batch: 901/5631, lr:6.25e-06, loss: 2.618823528289795 =====\n",
      "2021-03-27 21:46:49 ===== epoch: 1, batch: 921/5631, lr:6.25e-06, loss: 2.8883326053619385 =====\n",
      "2021-03-27 21:47:14 ===== epoch: 1, batch: 941/5631, lr:6.25e-06, loss: 1.1947394609451294 =====\n",
      "2021-03-27 21:47:39 ===== epoch: 1, batch: 961/5631, lr:6.25e-06, loss: 0.747903048992157 =====\n",
      "2021-03-27 21:48:04 ===== epoch: 1, batch: 981/5631, lr:6.25e-06, loss: 3.1981141567230225 =====\n",
      "2021-03-27 21:48:29 ===== epoch: 1, batch: 1001/5631, lr:6.25e-06, loss: 2.8617401123046875 =====\n",
      "2021-03-27 21:48:54 ===== epoch: 1, batch: 1021/5631, lr:6.25e-06, loss: 2.156052350997925 =====\n",
      "2021-03-27 21:49:19 ===== epoch: 1, batch: 1041/5631, lr:6.25e-06, loss: 0.6288976669311523 =====\n",
      "2021-03-27 21:49:43 ===== epoch: 1, batch: 1061/5631, lr:6.25e-06, loss: 1.046307921409607 =====\n",
      "2021-03-27 21:50:08 ===== epoch: 1, batch: 1081/5631, lr:6.25e-06, loss: 1.4967451095581055 =====\n",
      "2021-03-27 21:50:33 ===== epoch: 1, batch: 1101/5631, lr:6.25e-06, loss: 1.1532104015350342 =====\n",
      "2021-03-27 21:50:58 ===== epoch: 1, batch: 1121/5631, lr:6.25e-06, loss: 2.026594400405884 =====\n",
      "2021-03-27 21:51:23 ===== epoch: 1, batch: 1141/5631, lr:6.25e-06, loss: 2.0916616916656494 =====\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-b888f18a64ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-11-7df26906a68e>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mregion_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregion_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/YW/lib/python3.9/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/YW/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "novel-roots",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(epoch):\n",
    "    def truths_length(truths):\n",
    "        for i in range(50):\n",
    "            if truths[i][1] == 0:\n",
    "                return i\n",
    "    test_dataset = dataset.listDataset(basepath, testlist, dataset_use=dataset_use, shape=(init_width, init_height),\n",
    "                                       shuffle=False,\n",
    "                                       transform=transforms.Compose([\n",
    "                                           transforms.ToTensor()\n",
    "                                       ]), train=False)\n",
    "    test_rs = RandomSampler(test_dataset, replacement=True, num_samples=int(len(test_dataset)/n_sample_from))\n",
    "    test_loader = torch.utils.data.DataLoader(test_dataset, sampler=test_rs,\n",
    "                                              batch_size=batch_size, shuffle=False, **kwargs)\n",
    "\n",
    "    num_classes = region_loss.num_classes\n",
    "    anchors     = region_loss.anchors\n",
    "    num_anchors = region_loss.num_anchors\n",
    "    conf_thresh_valid = 0.005\n",
    "    total       = 0.0\n",
    "    proposals   = 0.0\n",
    "    correct     = 0.0\n",
    "    fscore = 0.0\n",
    "\n",
    "    correct_classification = 0.0\n",
    "    total_detected = 0.0\n",
    "\n",
    "    nbatch      = len(test_loader) # file_lines(testlist) // batch_size\n",
    "\n",
    "    logging('validation at epoch %d' % (epoch))\n",
    "    model.eval()\n",
    "\n",
    "    for batch_idx, (frame_idx, data, target) in enumerate(test_loader):\n",
    "        if use_cuda:\n",
    "            data = data.cuda()\n",
    "        with torch.no_grad():\n",
    "            output = model(data).data\n",
    "            all_boxes = get_region_boxes(output, conf_thresh_valid, num_classes, anchors, num_anchors, 0, 1)\n",
    "            for i in range(output.size(0)):\n",
    "                boxes = all_boxes[i]\n",
    "                boxes = nms(boxes, nms_thresh)\n",
    "                if dataset_use == 'ucf101-24':\n",
    "                    detection_path = os.path.join('ucf_detections', 'detections_'+str(epoch), frame_idx[i])\n",
    "                    current_dir = os.path.join('ucf_detections', 'detections_'+str(epoch))\n",
    "                    if not os.path.exists('ucf_detections'):\n",
    "                        os.mkdir('ucf_detections')\n",
    "                    if not os.path.exists(current_dir):\n",
    "                        os.mkdir(current_dir)\n",
    "                else:\n",
    "                    detection_path = os.path.join('jhmdb_detections', 'detections_'+str(epoch), frame_idx[i])\n",
    "                    current_dir = os.path.join('jhmdb_detections', 'detections_'+str(epoch))\n",
    "                    if not os.path.exists('jhmdb_detections'):\n",
    "                        os.mkdir('jhmdb_detections')\n",
    "                    if not os.path.exists(current_dir):\n",
    "                        os.mkdir(current_dir)\n",
    "\n",
    "                with open(detection_path, 'w+') as f_detect:\n",
    "                    for box in boxes:\n",
    "                        x1 = round(float(box[0]-box[2]/2.0) * 320.0)\n",
    "                        y1 = round(float(box[1]-box[3]/2.0) * 240.0)\n",
    "                        x2 = round(float(box[0]+box[2]/2.0) * 320.0)\n",
    "                        y2 = round(float(box[1]+box[3]/2.0) * 240.0)\n",
    "\n",
    "                        det_conf = float(box[4])\n",
    "                        for j in range((len(box)-5)//2):\n",
    "                            cls_conf = float(box[5+2*j].item())\n",
    "\n",
    "                            if type(box[6+2*j]) == torch.Tensor:\n",
    "                                cls_id = int(box[6+2*j].item())\n",
    "                            else:\n",
    "                                cls_id = int(box[6+2*j])\n",
    "                            prob = det_conf * cls_conf\n",
    "\n",
    "                            f_detect.write(str(int(box[6])+1) + ' ' + str(prob) + ' ' + str(x1) + ' ' + str(y1) + ' ' + str(x2) + ' ' + str(y2) + '\\n')\n",
    "                truths = target[i].view(-1, 5)\n",
    "                num_gts = truths_length(truths)\n",
    "        \n",
    "                total = total + num_gts\n",
    "    \n",
    "                for i in range(len(boxes)):\n",
    "                    if boxes[i][4] > 0.25:\n",
    "                        proposals = proposals+1\n",
    "\n",
    "                for i in range(num_gts):\n",
    "                    box_gt = [truths[i][1], truths[i][2], truths[i][3], truths[i][4], 1.0, 1.0, truths[i][0]]\n",
    "                    best_iou = 0\n",
    "                    best_j = -1\n",
    "                    for j in range(len(boxes)):\n",
    "                        iou = bbox_iou(box_gt, boxes[j], x1y1x2y2=False)\n",
    "                        if iou > best_iou:\n",
    "                            best_j = j\n",
    "                            best_iou = iou\n",
    "\n",
    "                    if best_iou > iou_thresh:\n",
    "                        total_detected += 1\n",
    "                        if int(boxes[best_j][6]) == box_gt[6]:\n",
    "                            correct_classification += 1\n",
    "\n",
    "                    if best_iou > iou_thresh and int(boxes[best_j][6]) == box_gt[6]:\n",
    "                        correct = correct+1\n",
    "\n",
    "            precision = 1.0*correct/(proposals+eps)\n",
    "            recall = 1.0*correct/(total+eps)\n",
    "            fscore = 2.0*precision*recall/(precision+recall+eps)\n",
    "            logging(\"[%d/%d] precision: %f, recall: %f, fscore: %f\" % (batch_idx, nbatch, precision, recall, fscore))\n",
    "\n",
    "    classification_accuracy = 1.0 * correct_classification / (total_detected + eps)\n",
    "    locolization_recall = 1.0 * total_detected / (total + eps)\n",
    "\n",
    "    logging(\"Classification accuracy: %.3f\" % classification_accuracy)\n",
    "    logging(\"Localization recall: %.3f\" % locolization_recall)\n",
    "\n",
    "    return fscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "posted-indonesia",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-03-26 21:23:35 validation at epoch 0\n",
      "2021-03-26 21:23:51 [0/765] precision: 0.999999, recall: 0.666666, fscore: 0.799995\n",
      "2021-03-26 21:23:54 [1/765] precision: 1.049999, recall: 0.724138, fscore: 0.857138\n",
      "2021-03-26 21:23:56 [2/765] precision: 1.000000, recall: 0.604651, fscore: 0.753618\n",
      "2021-03-26 21:23:58 [3/765] precision: 1.000000, recall: 0.561403, fscore: 0.719096\n",
      "2021-03-26 21:24:01 [4/765] precision: 1.162162, recall: 0.589041, fscore: 0.781814\n",
      "2021-03-26 21:24:04 [5/765] precision: 1.204545, recall: 0.602273, fscore: 0.803026\n",
      "2021-03-26 21:24:07 [6/765] precision: 1.234042, recall: 0.568627, fscore: 0.778519\n",
      "2021-03-26 21:24:09 [7/765] precision: 1.203703, recall: 0.570175, fscore: 0.773805\n",
      "2021-03-26 21:24:12 [8/765] precision: 1.228070, recall: 0.551181, fscore: 0.760865\n",
      "2021-03-26 21:24:14 [9/765] precision: 1.311475, recall: 0.563380, fscore: 0.788173\n",
      "2021-03-26 21:24:17 [10/765] precision: 1.343283, recall: 0.566038, fscore: 0.796456\n",
      "2021-03-26 21:24:19 [11/765] precision: 1.382353, recall: 0.549708, fscore: 0.786607\n",
      "2021-03-26 21:24:22 [12/765] precision: 1.430555, recall: 0.556757, fscore: 0.801552\n",
      "2021-03-26 21:24:25 [13/765] precision: 1.472973, recall: 0.545000, fscore: 0.795616\n",
      "2021-03-26 21:24:27 [14/765] precision: 1.487179, recall: 0.544601, fscore: 0.797247\n",
      "2021-03-26 21:24:29 [15/765] precision: 1.463414, recall: 0.533333, fscore: 0.781755\n",
      "2021-03-26 21:24:31 [16/765] precision: 1.453488, recall: 0.525210, fscore: 0.771601\n",
      "2021-03-26 21:24:34 [17/765] precision: 1.431579, recall: 0.535433, fscore: 0.779366\n",
      "2021-03-26 21:24:36 [18/765] precision: 1.425742, recall: 0.537313, fscore: 0.780484\n",
      "2021-03-26 21:24:37 [19/765] precision: 1.388889, recall: 0.530035, fscore: 0.767259\n",
      "2021-03-26 21:24:38 [20/765] precision: 1.382609, recall: 0.526490, fscore: 0.762586\n",
      "2021-03-26 21:24:39 [21/765] precision: 1.349593, recall: 0.523659, fscore: 0.754541\n",
      "2021-03-26 21:24:41 [22/765] precision: 1.301470, recall: 0.531532, fscore: 0.754793\n",
      "2021-03-26 21:24:41 [23/765] precision: 1.328467, recall: 0.526012, fscore: 0.753619\n",
      "2021-03-26 21:24:43 [24/765] precision: 1.340425, recall: 0.526462, fscore: 0.755996\n",
      "2021-03-26 21:24:44 [25/765] precision: 1.313333, recall: 0.525333, fscore: 0.750472\n",
      "2021-03-26 21:24:45 [26/765] precision: 1.301282, recall: 0.517857, fscore: 0.740872\n",
      "2021-03-26 21:24:46 [27/765] precision: 1.331210, recall: 0.517327, fscore: 0.745094\n",
      "2021-03-26 21:24:47 [28/765] precision: 1.358025, recall: 0.525060, fscore: 0.757311\n",
      "2021-03-26 21:24:48 [29/765] precision: 1.355422, recall: 0.519630, fscore: 0.751248\n",
      "2021-03-26 21:24:49 [30/765] precision: 1.358823, recall: 0.516779, fscore: 0.748780\n",
      "2021-03-26 21:24:50 [31/765] precision: 1.356322, recall: 0.513043, fscore: 0.744475\n",
      "2021-03-26 21:24:52 [32/765] precision: 1.366667, recall: 0.517895, fscore: 0.751141\n",
      "2021-03-26 21:24:53 [33/765] precision: 1.352941, recall: 0.516327, fscore: 0.747411\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-9ef624c49652>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-23-c2cd62455faf>\u001b[0m in \u001b[0;36mtest\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m                 \u001b[0mboxes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdataset_use\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ucf101-24'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m                     \u001b[0mdetection_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ucf_detections'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'detections_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/ylwu/YOWO/utils.py\u001b[0m in \u001b[0;36mnms\u001b[0;34m(boxes, nms_thresh)\u001b[0m\n\u001b[1;32m    163\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mboxes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0mbox_j\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mboxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msortIds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mbbox_iou\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbox_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbox_j\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx1y1x2y2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mnms_thresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                     \u001b[0;31m#print(box_i, box_j, bbox_iou(box_i, box_j, x1y1x2y2=False))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mbox_j\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd002/home/ylwu/YOWO/utils.py\u001b[0m in \u001b[0;36mbbox_iou\u001b[0;34m(box1, box2, x1y1x2y2)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mh2\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0muh\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mcarea\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mcw\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mch\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "prostate-lighting",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-constitution",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
